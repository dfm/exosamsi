\documentclass[12pt,letterpaper]{article}

% close up itemize
\renewenvironment{itemize}{\begin{list}{$\bullet$}{%
  \setlength{\topsep}{0.0ex}%
  \setlength{\parsep}{0.0ex}%
  \setlength{\partopsep}{0.0ex}%
  \setlength{\itemsep}{0.0ex}%
  \setlength{\leftmargin}{1.5\parindent}}}{\end{list}}

% text macros
\newcommand{\warning}[1]{\texttt{#1}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\Kepler}{\project{Kepler}}

% math operators
\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\elmult}{\ast}

% math macros
\newcommand{\datum}{Y}
\newcommand{\data}{\setof{\datum_n}_{n=1}^N}
\newcommand{\datavec}{y}
\renewcommand{\time}{t}
\newcommand{\exofn}{Q}
\newcommand{\exovec}{q}
\newcommand{\exopars}{\omega}
\newcommand{\flux}{F}
\newcommand{\meanflux}{\bar{f}}
\newcommand{\starpars}{\alpha}
\newcommand{\noise}{e}
\newcommand{\variance}{\sigma^2}
\newcommand{\hyperpars}{\varphi}
\newcommand{\normal}{{\mathcal N}}
\newcommand{\mean}{\mu}
\newcommand{\meanvec}{m}
\newcommand{\wavevec}{a}
\newcommand{\noisetensor}{\Psi}
\newcommand{\wavetensor}{\Phi}
\newcommand{\weightmatrix}{W}
\newcommand{\vartensor}{V}
\newcommand{\onevec}{O}

\begin{document}\sloppy\sloppypar

\section*{A partially marginalized likelihood for exoplanet inference}

\noindent
David W. Hogg (NYU) \\
\textit{with help from Paul Baines (Davis) and everyone else at SAMSI \Kepler}

\paragraph{disclaimer:}
\warning{This document is a draft, and not yet ready for public consumption.
In particular, any use of the content in this document might represent a serious lapse in judgement.}

\paragraph{abstract:}
We present a likelihood function for the time-series photometry
that makes up a lightcurve from the \Kepler\ mission (or similar experiment).
The function includes parameters that describe stochastic stellar variability
and parameters that describe periodic exoplanet (or other companion) transits.
The stellar variability model is a Gaussian Process in a wavelet basis
that is capable of modeling non-trivial time correlations with a diagonal covariance matrix.
These choices make it possible to \emph{marginalize out} all stellar-variability parameters,
leaving the user with a flexible likelihood function parameterized only by exoplanet parameters.
We show \warning{something non-trivial}.

\section{generalities}

There are $N$ observations $\datum_n$ of a single star taken at times $\time_n$.
The model is
\begin{eqnarray}
\datum_n &=& [1 - \exofn(t_n\given\exopars)]\,\flux(t_n\given\starpars) + \noise_n
\quad ,
\end{eqnarray}
where
$\exofn(\cdot\given\exopars)$ is a function
that describes the attenuation of the starlight caused by the transiting exoplanet,
$\exopars$ is a blob of parameters describing an exoplanet's size and orbit,
$\flux(\cdot\given\starpars)$ is a function
that describes the apparent brightness of the star as a function of time,
$\starpars$ is a blob of parameters describing the stellar mean flux and variability,
and $\noise_n$ is the noise contribution to the $n$th datum
(coming from photon and read noise, among other things).
This description---plus a model for the noise---will lead to a justifiable likelihood function.
If we model the noise contributions as being Gaussian and independent
with known variances $\variance_n$, the likelihood function becomes
\begin{eqnarray}
p(\data\given\exopars,\starpars,\hyperpars)
  &=& \prod_{n=1}^N p(\datum_n\given\exopars,\starpars,\hyperpars)
\label{eq:like}
\\
p(\datum_n\given\exopars,\starpars,\hyperpars)
  &=& \normal(\datum_n\given\mean_n,\variance_n)
\\
\mean_n
  &\equiv& [1 - \exofn(t_n\given\exopars)]\,\flux(t_n\given\starpars)
\quad ,
\end{eqnarray}
where
$\data$ is the set of all data,
$\hyperpars$ is an enormous blob of hyperparameters
that includes all our decision-making and assumptions (and more, soon),
the product in the likelihood encodes our ``independent noise'' assumption,
and $\normal(x\given m,V)$ is the Gaussian for $x$ with mean $m$ and variance $V$,

By assumption, we care only about the exoplanet parameters $\exopars$
and not in the least about the star parameters $\starpars$.
We marginalize by
\begin{eqnarray}
p(\data\given\exopars,\hyperpars)
  &=& \int p(\data\given\exopars,\starpars,\hyperpars)\,p(\starpars\given\hyperpars)\,\dd\starpars
\label{eq:mlike}
\quad ,
\end{eqnarray}
where
we have had to introduce a prior PDF $p(\starpars\given\hyperpars)$ for the star parameters.
This prior will in general depend on the hyperparameters $\hyperpars$,
which is a very good thing (because learning can happen there).
The object defined in (\ref{eq:mlike}) is the \emph{partially marginalized likelihood}
we seek.

The considerations \textsl{(1)}~that marginalization is paramount,
and \textsl{(2)}~that the variability of the star is stochastic,
lead us naturally to think about Gaussian Processes.
In a Gaussian process in this context,
there are hyperparameters controlling a non-trivial covariance matrix in the space of the data,
and the star is modeled (in a prior sense) as a Gaussian draw from this covariance matrix.
The brilliant idea generated at SAMSI is that we can model a non-trivial covariance matrix,
which will be dense in the original data space,
with a trivial, \emph{diagonal} covariance matrix in a different basis.
For all sorts of good reasons, we will think about wavelet bases,
but what follows is pretty general.

Because we are about to get all linear-algebra-y, let's make some
notation changes: Instead of the ``set'' of data $\data$ we will move
to thinking about the $N$ data points as components of a column vector
$\datavec$.  Instead of the ``function'' $\exofn(\cdot\given\exopars)$,
we will think of the column vector $\exovec$.
Instead of individual scalar noise variances,
we will think of variance tensors.

We are going to transform to another basis;
that is, we are going to ``rotate'' between the time basis in which vector $y$ lives
to a basis-function-amplitude vector $\wavevec$ wavelet-amplitude by a transformation like
\begin{eqnarray}
\datavec
  &\leftarrow& \weightmatrix\cdot\wavevec
\quad ,
\end{eqnarray}
where $\weightmatrix$ is a matrix of ``weights'' or basis functions,
and $\wavevec$ is a vector of basis-function amplitudes.
If this transformation is unitary, the inverse is
\begin{eqnarray}
\wavevec
  &\leftarrow& \transpose{\weightmatrix}\cdot\datavec
\quad .
\end{eqnarray}
In the real world, to make the transformation unitary,
the user either must have evenly spaced, homoskedastic data (all $\variance_n$ identical),
or else build a unique, inhomogeneous wavelet basis
customized to every non-uniform, heteroskedastic data set or time series.

In this linear algebra context,
we can write a partially (very partially; see below) marginalized likelihood as:
\begin{eqnarray}
p(\datavec\given\exopars,\starpars,\hyperpars)
  &=& \normal(\datavec\given\meanvec,\vartensor)
\label{eq:wavelike}
\\
\meanvec
  &\equiv& \meanflux\,[\onevec - \exovec]
\\
\vartensor
  &\equiv& \noisetensor + \weightmatrix\cdot\wavetensor\cdot\transpose{\weightmatrix}
\label{eq:variance}
\quad ,
\end{eqnarray}
where
$\meanvec$ and $\vartensor$ are the mean vector and variance tensor of the Gaussian Process,
$\meanflux$ is the mean stellar flux,
$\onevec$ is the $N$-dimensional vector of ones (unities),
$\exovec$ is the column vector made up of the $\exofn(t_n\given\exopars)$ values,
$\noisetensor$ is the diagonal noise variance tensor with $\variance_n$ values down the diagonal,
$\weightmatrix$ is the linear operator that transforms points
from the (natural) time basis to the wavelet basis,
and $\wavetensor$ is the diagonal tensor of variances appropriate to the wavelet components.
That is, down the diagonal of $\wavetensor$ are the variances from which,
in a prior sense,
the amplitudes of the wavelets are expected to be drawn,
in the absence of (or prior to) data.
Another way to put it is that the variance tensor $\vartensor$ is dense,
but it is made from the two diagonal tensors $\noisetensor$ and $\wavetensor$,
which are diagonal in different bases.  Some notes:
\begin{itemize}
\item
Although we refer to the expression in (\ref{eq:wavelike}) as
a partially marginalized likelihood, it still contains the stellar parameter blob $\starpars$.
This is because the variances along the diagonal of $\wavetensor$ are parameters of the star model;
they are specific to the star.
The partial marginalization has happened over the specific phases or amplitudes of the wavelet components,
but not over the variances describing the distribution from which they were drawn.
\item
With an additional prior $p(\wavetensor\given\hyperpars)$,
the star model can be marginalized out.
This will indeed become our goal, below.
\warning{This needs to be spelled out here or above and made explicit.}
\item
The wavelet amplitude prior variance $\wavetensor$ does \emph{not} need to be diagonal.
It could be sparse, or tri-diagonal, or dense.
Of course if it is fully dense, the advantage of going to the wavelet basis is reduced.
\item
If we only use the weight matrix $\weightmatrix$ to transform \emph{covariance matrices}---%
and not transform and transform back the data or residuals---%
we can in fact make the $\weightmatrix$ non-square,
both by removing rows (or columns?) that correspond to wavelet scales at which we expect no power
(more on this below),
and by removing columns (or rows?) that correspond to data locations at which there are no observations
(vanishing $1/\variance_n$).
Finally, If we drop columns (or rows?) that correspond to contiguous intervals of data locations,
there will be some ``unsupported'' wavelet components.
These orphaned rows (or columns?) ought to also be dropped.
These changes break orthonormality and completeness of the basis,
but they do not change the symbolic form (\ref{eq:variance}) of the total variance tensor.
\end{itemize}

We can make further assumptions or restrictions of this model,
capitalizing on the properties of the wavelet transform.
In particular, the wavelet transform produces basis functions that are
localized in both frequency and time.
Our assumption is that the variances along the diagonal of the wavelet variance tensor $\wavetensor$
are a function of frequency but \emph{not} time.
That is, we assume that within each freqency band,
the wavelet coefficients are drawn iid.

In what follows,
we are going to treat the diagonal data noise variance tensor $\noisetensor$ as known,
the wavelet basis $\weightmatrix$ as fixed,
but parameterize and permit to be fit the Gaussian Process variance tensor $\wavetensor$.

\section{\Kepler\ specifics}

My proposal---it can't be described as ``our proposal'' just yet---%
is to do the following violence to the model to keep the core ideas,
but make it tractable, at the cost of strengthening or distorting some assumptions.
\begin{itemize}
\item
For the purposes of computing the wavelet transformation matrices $\weightmatrix$,
treat the data as coming homoskedastic on a perfectly equally-spaced grid.
This will ensure that the $\weightmatrix$ matrices are always unitary.
This isn't required for inference but will be \emph{sick} for performance.
\item
An alternative is to work on the data in month-long segments
(within which the data \emph{are} very close to uniformly sampled in time).
The information in the data about the wavelet amplitude prior variance $\wavetensor$
would have to be pooled across segments, but each segment would be transformed independently.
\item
At the same time as we treat the data as uniform in cadence and homoskedastic for the purposes
of the $\weightmatrix$, for the purposes of computing the \emph{exoplanet transits},
treat the data as coming from the true, non-uniform grid in BJD.
\item
At the same time as we treat the data as uniform in cadence and homoskedastic for the purposes
of the $\weightmatrix$, for the purposes of computing the likelihood function,
use the heteroskedastic $\noisetensor$.
\end{itemize}

\end{document}
