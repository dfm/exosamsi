\documentclass[12pt,letterpaper]{article}

% close up itemize
\renewenvironment{itemize}{\begin{list}{$\bullet$}{%
  \setlength{\topsep}{0.0ex}%
  \setlength{\parsep}{0.0ex}%
  \setlength{\partopsep}{0.0ex}%
  \setlength{\itemsep}{0.0ex}%
  \setlength{\leftmargin}{1.5\parindent}}}{\end{list}}

% text macros
\newcommand{\warning}[1]{\texttt{#1}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\Kepler}{\project{Kepler}}

% math operators
\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\elmult}{\ast}

% math macros
\newcommand{\datum}{Y}
\newcommand{\data}{\setof{\datum_n}_{n=1}^N}
\newcommand{\datavec}{y}
\renewcommand{\time}{t}
\newcommand{\exofn}{Q}
\newcommand{\exovec}{q}
\newcommand{\exopars}{\omega}
\newcommand{\flux}{F}
\newcommand{\meanflux}{\bar{f}}
\newcommand{\lcpars}{\alpha}
\newcommand{\noise}{e}
\newcommand{\variance}{\sigma^2}
\newcommand{\hyperpars}{\varphi}
\newcommand{\normal}{{\mathcal N}}
\newcommand{\mean}{\mu}
\newcommand{\meanvec}{m}
\newcommand{\wavevec}{a}
\newcommand{\noisetensor}{\Psi}
\newcommand{\wavetensor}{\Phi}
\newcommand{\weightmatrix}{W}
\newcommand{\vartensor}{V}
\newcommand{\basisfunction}{g}
\newcommand{\onevec}{O}

\begin{document}\sloppy\sloppypar

\section*{A partially marginalized likelihood for exoplanet inference}

\noindent
David W. Hogg (NYU) \\
\textit{with help from Paul Baines (Davis) and everyone else at SAMSI \Kepler}

\paragraph{disclaimer:}
\warning{This document is a draft, and not yet ready for public consumption.
In particular, any use of the content in this document might represent a serious lapse in judgement.}

\paragraph{abstract:}
We present a likelihood function for the time-series photometry
that makes up a lightcurve from the \Kepler\ mission (or similar experiment).
The function includes parameters that describe stochastic stellar variability
and parameters that describe periodic exoplanet (or other companion) transits.
The stellar variability model is a Gaussian Process in a wavelet basis
that is capable of modeling non-trivial time correlations with a diagonal covariance matrix.
These choices make it possible to \emph{marginalize out} all stellar-variability parameters,
leaving the user with a flexible likelihood function parameterized only by exoplanet parameters.
We show \warning{something non-trivial}.

\section{Introduction}

\warning{Transits are hard to find in general; harder in variable stars.  All stars are variable.}

\warning{Fitting transits and stellar variability simultaneously is The Right Thing To Do~(tm).}

\warning{Gaussian Processes are a Good Idea~(tm).}

\warning{Wavelets are a Good Idea~(tm).}

\section{Likelihood generalities}

There are $N$ observations $\datum_n$ of a single star taken at times $\time_n$.
The model is
\begin{eqnarray}
\datum_n &=& [1 - \exofn(t_n\given\exopars)]\,\flux(t_n\given\lcpars) + \noise_n
\quad ,
\end{eqnarray}
where
$\exofn(\cdot\given\exopars)$ is a function
that describes the attenuation of the starlight caused by the transiting exoplanet,
$\exopars$ is a blob of parameters describing an exoplanet's size and orbit,
$\flux(\cdot\given\lcpars)$ is a function
that describes the apparent brightness of the star as a function of time,
$\lcpars$ is a blob of parameters describing the stellar mean flux and the
variation of that flux with time,
and $\noise_n$ is the noise contribution to the $n$th datum
(coming from photon and read noise, among other things).
This description---plus a model for the noise---will lead to a justifiable likelihood function.
If we model the noise contributions as being Gaussian and independent
with known variances $\variance_n$, the likelihood function becomes
\begin{eqnarray}
p(\data\given\exopars,\lcpars,\hyperpars)
  &=& \prod_{n=1}^N p(\datum_n\given\exopars,\lcpars,\hyperpars)
\label{eq:like}
\\
p(\datum_n\given\exopars,\lcpars,\hyperpars)
  &=& \normal(\datum_n\given\mean_n,\variance_n)
\\
\mean_n
  &\equiv& [1 - \exofn(t_n\given\exopars)]\,\flux(t_n\given\lcpars)
\quad ,
\end{eqnarray}
where
$\data$ is the set of all data,
$\hyperpars$ is an enormous blob of hyperparameters
that includes all our decision-making and assumptions (and more, soon),
the product in the likelihood encodes our ``independent noise'' assumption,
and $\normal(x\given m,V)$ is the Gaussian for $x$ with mean $m$ and variance $V$,

By assumption, we care only about the exoplanet parameters $\exopars$
and not in the least about the stellar variability parameters $\lcpars$.
We marginalize by
\begin{eqnarray}
p(\data\given\exopars,\hyperpars)
  &=& \int p(\data\given\exopars,\lcpars,\hyperpars)\,p(\lcpars\given\hyperpars)\,\dd\lcpars
\label{eq:mlike}
\quad ,
\end{eqnarray}
where
we have had to introduce a prior PDF $p(\lcpars\given\hyperpars)$ for the variability parameters.
This prior will in general depend on the hyperparameters $\hyperpars$
(which is a very good thing (because learning can happen there).
The object defined in (\ref{eq:mlike}) is the \emph{partially marginalized likelihood}
we seek.

\section{Wavelet-based Gaussian Process}

The considerations \textsl{(1)}~that marginalization is paramount,
and \textsl{(2)}~that the variability of the star is stochastic,
lead us naturally to think about Gaussian Processes.
In a Gaussian process in this context,
there are hyperparameters controlling a non-trivial covariance matrix in the space of the data,
and the star is modeled (in a prior sense) as a Gaussian draw from this covariance matrix.
The brilliant idea generated at SAMSI is that we can model a non-trivial covariance matrix,
which will be dense in the original data space,
with a trivial, \emph{diagonal} covariance matrix in a different basis.
For all sorts of good reasons, we will think about wavelet bases,
but what follows is pretty general.

Because we are about to get all linear-algebra-y, let's make some
notation changes: Instead of the ``set'' of data $\data$ we will move
to thinking about the $N$ data points as components of a column vector
$\datavec$.  Instead of the ``function'' $\exofn(\cdot\given\exopars)$,
we will think of the column vector $\exovec$.
Instead of individual scalar noise variances,
we will think of variance tensors.

We are going to transform to another basis;
that is, we are going to ``rotate'' between the time basis in which vector $y$ lives
to a basis-function-amplitude vector $\wavevec$ wavelet-amplitude by a transformation like
\begin{eqnarray}
\datavec
  &\leftarrow& \transpose{\weightmatrix}\cdot\wavevec
\quad ,
\end{eqnarray}
where $\weightmatrix$ is a matrix of ``weights'' or basis functions,
and $\wavevec$ is a vector of basis-function amplitudes.
If this transformation is unitary, the inverse is
\begin{eqnarray}
\wavevec
  &\leftarrow& \weightmatrix\cdot\datavec
\quad .
\end{eqnarray}
In the real world, to make the transformation unitary,
the user either must have evenly spaced, homoskedastic data (all $\variance_n$ identical),
or else build a unique, inhomogeneous wavelet basis
customized to every non-uniform, heteroskedastic data set or time series.
In what follows, we won't need this unitarity to be true.

In this framework, the star variability parameters $\lcpars$%
---the parameters that set the particular shape of the star's flux as a function of time---%
are contained in the vector $\wavevec$ that comprises the amplitudes of the wavelet coefficients.
This model is equivalent to assuming that the star's flux as a function of time
can be written as a superposition of basis functions
\begin{eqnarray}
\flux(t\given\lcpars) &=& \sum_{k=1}^K \sum_{p=1}^{P_k} \wavevec_{kp}\,\basisfunction_{kp}(t)
\end{eqnarray}
where
the $\wavevec_{kp}$ are the elements of the vector $\wavevec$
(and therefore the contents of the parameter blob $\lcpars$),
and the $\basisfunction_{kp}$ are the wavelet basis functions.
The basis functions are indexed by two indices,
one ($k$) going over the frequency domain,
and the other ($p$) going over the location or time domain.

The two key ideas of the variability model we propose here%
---the model for $p(\lcpars\given\hyperpars)$ or equivalently $p(\wavevec\given\hyperpars)$---%
are \textsl{(1)}~that in the wavelet basis the Gaussian from which the vector $\wavevec$ is drawn
can have a diagonal covariance matrix,
and \textsl{(2)}~that the variances on the diagonal of that matrix will depend only on the
frequency index $k$ and not at all on the location index $p$.
Under these assumptions (really only the diagonality assumption),
we can write the partially marginalized likelihood as:
\begin{eqnarray}
p(\datavec\given\exopars,\hyperpars)
  &=& \normal(\datavec\given\meanvec,\vartensor)
\label{eq:wavelike}
\\
\meanvec
  &\equiv& \meanflux\,[\onevec - \exovec]
\\
\vartensor
  &\equiv& \noisetensor + \weightmatrix\cdot\transpose{\wavetensor}\cdot{\weightmatrix}
\label{eq:variance}
\quad ,
\end{eqnarray}
where
$\meanvec$ and $\vartensor$ are the mean vector and variance tensor of the Gaussian Process,
$\meanflux$ is the mean stellar flux,
$\onevec$ is the $N$-dimensional vector of ones (unities),
$\exovec$ is the column vector made up of the $\exofn(t_n\given\exopars)$ values,
$\noisetensor$ is the diagonal noise variance tensor with $\variance_n$ values down the diagonal,
$\weightmatrix$ is the linear operator that transforms points
from the (natural) time basis to the wavelet basis,
and $\wavetensor$ is the diagonal tensor of variances appropriate to the wavelet components.
That is, down the diagonal of $\wavetensor$ are the variances from which,
in a prior sense,
the amplitudes $\wavevec$ of the wavelet components are expected to be drawn,
in the absence of (or prior to) data.
Another way to put it is that the variance tensor $\vartensor$ is dense,
but it is made from the two diagonal tensors $\noisetensor$ and $\wavetensor$,
each of which is diagonal (but in different bases).  Some notes:

\textsl{(a)}~%
In this way of thinking about it, the wavelet-amplitude diagonal
variance tensor $\wavetensor$ is part of the hyperparameter blob
$\hyperpars$.
With an additional hyperparameters in this blob,
and choices about hyperpriors on the hyperpriors,
the variability model itself can be marginalized out.
This may indeed become one of our goals below.

\textsl{(b)}~%
The form (\ref{eq:variance}) for the variance tensor assumes diagonality
but does not yet capitalize on the frequency and location of the wavelets.
The idea that the variances will depend on the freqency index but \emph{not} the location index.
That means that diagonal tensor $\wavetensor$ will have repeated elements; the number of
free parameters will be far smaller than the number of wavelet amplitudes in $\wavevec$.

\textsl{(c)}~%
For many wavelet bases, the number $K$ of distinct frequencies $k$ is only of order $\log_2(N)$,
where $N$ is the number of parameters.
This means that even a non-parametric form for $\wavetensor$
will only grow logarithmically with the size of the data set.

\textsl{(d)}~%
The wavelet amplitude prior variance $\wavetensor$ does \emph{not} need to be diagonal in principle.
It could be sparse, or tri-diagonal, or dense.
Of course if it is fully dense, the advantage of going to the wavelet basis is reduced.
We won't consider that case further here.

\textsl{(e)}~%
If we only use the weight matrix $\weightmatrix$ to transform \emph{covariance matrices}---%
and not transform and transform back the data or residuals---%
we can in fact make the $\weightmatrix$ non-square,
both by removing rows that correspond to wavelet scales at which we expect no power
(more on this below),
and by removing columns that correspond to data locations at which there are no observations
(vanishing $1/\variance_n$).
Finally, If we drop columns that correspond to contiguous intervals of data locations,
there will in general be some ``unsupported'' wavelet components.
These orphaned rows ought to also be dropped to keep relevant matrices full-rank.
These changes break orthonormality and completeness of the basis,
but they do not change the symbolic form (\ref{eq:variance}) of the total variance tensor.

In what follows,
we are going to treat the diagonal data noise variance tensor $\noisetensor$ as known,
the wavelet basis $\weightmatrix$ as fixed,
but parameterize and permit to be fit the Gaussian Process variance tensor $\wavetensor$,
parameterized in a form that makes the variances sensitive to scale but not location.

\section{\Kepler\ specifics}

My proposal---it can't be described as ``our proposal'' just yet---%
is to do the following violence to the model to keep the core ideas,
but make it tractable, at the cost of strengthening or distorting some assumptions.

\warning{Going to work on the Untrendied SAP data.
         Explain why}

\warning{Turning Untrendy knobs to 11.
         We don't want any discontinuities or artifacts.}

For the purposes of computing the wavelet transformation matrices $\weightmatrix$,
treat the data as coming homoskedastic on a perfectly equally-spaced grid.
Then we will drop rows and columns where there is no data,
or no support for particular wavelet basis functions.

We will also punk (slightly) the wavelet transform in the following sense:
We will treat the data as uniform in cadence for the purposes
of the $\weightmatrix$, for the purposes of computing the \emph{exoplanet transits},
treat the data as coming from the true, non-uniform grid in BJD.
That is, we will be able to use ``out of the box'' code to generate the wavelet transform
matrix $\weightmatrix$.

\section{Search}

\warning{We have to say something about the horrible, horrible things we do to get started.}

\section{Experiments}

\warning{Experiment 1: Run on a single injection into a single good G star.
         Show that we recover correctly.}

\warning{Experiment 2: Run on a representative sample of G stars with a representative sample of injections.
         Show that we rock it.}

\warning{Experiment 3: Run on 10,000 G stars.
         Find all ELPaSLS.}

\section{Discussion}

\warning{foo}

\end{document}
