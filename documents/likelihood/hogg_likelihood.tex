\documentclass[12pt,letterpaper]{article}

\newcommand{\warning}[1]{\texttt{#1}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\Kepler}{\project{Kepler}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\elmult}{\ast}
\newcommand{\datum}{Y}
\newcommand{\data}{\setof{\datum_n}_{n=1}^N}
\newcommand{\datavec}{y}
\renewcommand{\time}{t}
\newcommand{\exofn}{Q}
\newcommand{\exovec}{q}
\newcommand{\exopars}{\omega}
\newcommand{\flux}{F}
\newcommand{\meanflux}{\bar{f}}
\newcommand{\starpars}{\alpha}
\newcommand{\noise}{e}
\newcommand{\variance}{\sigma^2}
\newcommand{\hyperpars}{\varphi}
\newcommand{\normal}{{\mathcal N}}
\newcommand{\mean}{\mu}
\newcommand{\meanvec}{m}
\newcommand{\wavevec}{a}
\newcommand{\noisetensor}{\Psi}
\newcommand{\wavetensor}{\Phi}
\newcommand{\weightmatrix}{W}
\newcommand{\vartensor}{V}
\newcommand{\onevec}{O}


\begin{document}\sloppy\sloppypar

\section*{A partially marginalized likelihood for exoplanet inference}

\noindent
David W. Hogg (NYU) \\
\textit{with help from Paul Baines (Davis) and everyone else at SAMSI \Kepler}

\paragraph{disclaimer:}
\warning{This document is a draft, and not yet ready for public consumption.
In particular, any use of the content in this document might represent a serious lapse in judgement.}

\paragraph{abstract:}
We present a likelihood function for the time-series photometry
that makes up a lightcurve from the \Kepler\ mission (or similar experiment).
The function includes parameters that describe stochastic stellar variability
and parameters that describe periodic exoplanet (or other companion) transits.
The stellar variability model is a Gaussian Process in a wavelet basis
that is capable of modeling non-trivial time correlations with a diagonal covariance matrix.
These choices make it possible to \emph{marginalize out} all stellar-variability parameters,
leaving the user with a flexible likelihood function parameterized only by exoplanet parameters.
We show \warning{something non-trivial}.

\section{generalities}

There are $N$ observations $\datum_n$ of a single star taken at times $\time_n$.
The model is
\begin{eqnarray}
\datum_n &=& [1 - \exofn(t_n\given\exopars)]\,\flux(t_n\given\starpars) + \noise_n
\quad ,
\end{eqnarray}
where
$\exofn(\cdot\given\exopars)$ is a function
that describes the attenuation of the starlight caused by the transiting exoplanet,
$\exopars$ is a blob of parameters describing an exoplanet's size and orbit,
$\flux(\cdot\given\starpars)$ is a function
that describes the apparent brightness of the star as a function of time,
$\starpars$ is a blob of parameters describing the stellar mean flux and variability,
and $\noise_n$ is the noise contribution to the $n$th datum
(coming from photon and read noise, among other things).
This description---plus a model for the noise---will lead to a justifiable likelihood function.
If we model the noise contributions as being Gaussian and independent
with known variances $\variance_n$, the likelihood function becomes
\begin{eqnarray}
p(\data\given\exopars,\starpars,\hyperpars)
  &=& \prod_{n=1}^N p(\datum_n\given\exopars,\starpars,\hyperpars)
\label{eq:like}
\\
p(\datum_n\given\exopars,\starpars,\hyperpars)
  &=& \normal(\datum_n\given\mean_n,\variance_n)
\\
\mean_n
  &\equiv& [1 - \exofn(t_n\given\exopars)]\,\flux(t_n\given\starpars)
\quad ,
\end{eqnarray}
where
$\data$ is the set of all data,
$\hyperpars$ is an enormous blob of hyperparameters
that includes all our decision-making and assumptions (and more, soon),
the product in the likelihood encodes our ``independent noise'' assumption,
and $\normal(x\given m,V)$ is the Gaussian for $x$ with mean $m$ and variance $V$,

By assumption, we care only about the exoplanet parameters $\exopars$
and not in the least about the star parameters $\starpars$.
We marginalize by
\begin{eqnarray}
p(\data\given\exopars,\hyperpars)
  &=& \int p(\data\given\exopars,\starpars,\hyperpars)\,p(\starpars\given\hyperpars)\,\dd\starpars
\label{eq:mlike}
\quad ,
\end{eqnarray}
where
we have had to introduce a prior PDF $p(\starpars\given\hyperpars)$ for the star parameters.
This prior will in general depend on the hyperparameters $\hyperpars$,
which is a very good thing (because learning can happen there).
The object defined in (\ref{eq:mlike}) is the \emph{partially marginalized likelihood}
we seek.

The considerations \textsl{(1)}~that marginalization is paramount,
and \textsl{(2)}~that the variability of the star is stochastic,
lead us naturally to think about Gaussian Processes.
In a Gaussian process in this context,
there are hyperparameters controlling a non-trivial covariance matrix in the space of the data,
and the star is modeled (in a prior sense) as a Gaussian draw from this covariance matrix.
The brilliant idea generated at SAMSI is that we can model a non-trivial covariance matrix,
which will be dense in the original data space,
with a trivial, \emph{diagonal} covariance matrix in a different basis.
For all sorts of good reasons, we will think about wavelet bases,
but what follows is pretty general.

Because we are about to get all linear-algebra-y, let's make some
notation changes: Instead of the ``set'' of data $\data$ we will move
to thinking about the $N$ data points as components of a column vector
$\datavec$.  Instead of the ``function'' $\exofn(\cdot\given\exopars)$,
we will think of the column vector $\exovec$.
Instead of individual scalar noise variances,
we will think of variance tensors.

We are going to transform to another basis;
that is, we are going to ``rotate'' between the time basis in which vector $y$ lives
to a basis-function-amplitude vector $\wavevec$ wavelet-amplitude by a transformation like
\begin{eqnarray}
\datavec
  &\leftarrow& \weightmatrix\cdot\wavevec
\quad ,
\end{eqnarray}
where $\weightmatrix$ is a matrix of ``weights'' or basis functions,
and $\wavevec$ is a vector of basis-function amplitudes.
If this transformation is unitary, the inverse is
\begin{eqnarray}
\wavevec
  &\leftarrow& \transpose{\weightmatrix}\cdot\datavec
\quad .
\end{eqnarray}
In the real world, to make the transformation unitary,
the user either must have evenly spaced, homoskedastic data (all $\variance_n$ identical),
or else build a unique, inhomogeneous wavelet basis
customized to every non-uniform, heteroskedastic data set or time series.

In this linear algebra context, we can write the unmarginalized likelihood (\ref{eq:like}) as
\begin{eqnarray}
p(\datavec\given\exopars,\starpars,\hyperpars)
  &=& \normal(\datavec\given\meanvec,\vartensor)
\\
\meanvec
  &\equiv& \meanflux\,[\onevec - \exovec]
\\
\vartensor
  &\equiv& \noisetensor + \weightmatrix\cdot\wavetensor\cdot\transpose{\weightmatrix}
\quad ,
\end{eqnarray}
where
$\meanvec$ and $\vartensor$ are the mean vector and variance tensor of the Gaussian Process,
$\meanflux$ is the mean stellar flux,
$\onevec$ is the $N$-dimensional vector of ones (unities),
$\exovec$ is the column vector made up of the $\exofn(t_n\given\exopars)$ values,
$\noisetensor$ is the diagonal noise variance tensor with $\variance_n$ values down the diagonal,
$\weightmatrix$ is the linear operator that transforms points
from the (natural) time basis to the wavelet basis,
and $\wavetensor$ is the diagonal tensor of variances appropriate to the wavelet components.
That is, down the diagonal of $\wavetensor$ are the variances from which,
in a prior sense,
the amplitudes of the wavelets are expected to be drawn,
in the absence of (or prior to) data.
Another way to put it is that the variance tensor $\vartensor$ is dense,
but it is made from the two diagonal tensors $\noisetensor$ and $\wavetensor$,
which are diagonal in different bases.

In what follows,
we are going to treat the diagonal data noise variance tensor $\noisetensor$ as known,
the wavelet basis $\weightmatrix$ as fixed,
but parameterize and permit to be fit the Gaussian Process variance tensor $\wavetensor$.

\section{\Kepler specifics}

\warning{hoo haw}

\end{document}
